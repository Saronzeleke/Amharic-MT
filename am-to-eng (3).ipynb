{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12496838,"sourceType":"datasetVersion","datasetId":7886696}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport csv\nfrom datasets import load_dataset, Dataset\nimport io\nimport os\nimport random\nimport sys\nfrom tokenizers import Tokenizer\nimport torch\nfrom transformers import AutoTokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:13:33.332881Z","iopub.execute_input":"2025-09-09T15:13:33.333489Z","iopub.status.idle":"2025-09-09T15:14:11.024111Z","shell.execute_reply.started":"2025-09-09T15:13:33.333464Z","shell.execute_reply":"2025-09-09T15:14:11.023535Z"}},"outputs":[{"name":"stderr","text":"2025-09-09 15:13:54.285035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757430834.628363      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757430834.719265      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#configuration.py\nbase_path = \"/kaggle/input/cac-da-machine-translation-corpus-eng-amh\"\nwhole_tsv = f\"{base_path}/CAC_DA_Amh_Eng_whole_data.tsv\"\ntrain_data  = f\"{base_path}/CAC_DA_Amh_Eng_train.tsv\"\nval_data   = f\"{base_path}/CAC_DA_Amh_Eng_val.tsv\"\ntest_data   = f\"{base_path}/CAC_DA_Amh_Eng_test.tsv\"\n\nlog_file   = \"/kaggle/working/output/malformed_lines.log\"\npretrained_tokenizer_name = 'facebook/nllb-200-distilled-600M'\npretrained_tokenizer = f'{base_path}nllb_200_distilled_600M_tokenizer'\noutput_dir = '/kaggle/working/output/nllb_finetuned'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nsplit_ratio = (0.84, 0.08, 0.08)\nmin_len = 1  # adjust to your preference\nmax_len = 128  # Maximum length for tokenization\n\nbatch_size = 8\nnum_epochs = 100\n\ntag_am, tag_en = \">>amh<<\", \">>eng<<\"\npairs_written, malformed = 0, 0\n# make sure log_file is created\nos.makedirs(log_file, exist_ok=True)\ndef is_ok(src, tgt):\n    # Basic filtering: skip very short or malformed lines\n    if src is None or tgt is None:\n        return False\n    if len(src.split()) < 3 or len(tgt.split()) < 3:\n        return False\n    return True\n\nclass TSVStream:\n    def __init__(self, path, tokenizer):\n        self.path = path\n        self.tok = tokenizer\n\n    def __iter__(self):\n        ds = load_dataset(\"csv\", data_files=self.path, split=\"train\", delimiter=\"\\t\",\n                          column_names=[\"tag\", \"src\", \"tgt\"], quoting=csv.QUOTE_MINIMAL, streaming=True, )\n\n        lang_token_map = {\">>eng<<\": \"eng_Latn\", \">>amh<<\": \"amh_Ethi\" }\n\n        for ex in ds:\n            if not is_ok(ex[\"src\"], ex[\"tgt\"]):\n                continue\n\n            tag = ex[\"tag\"]\n            lang_code = lang_token_map.get(tag)\n            if lang_code is None:\n                continue\n\n            forced_bos_token_id = self.tok.convert_tokens_to_ids(lang_code)\n            if forced_bos_token_id is None:\n                continue\n\n            # ðŸ”¥ Set the target language code manually\n            self.tok.tgt_lang = lang_code\n\n            source = f\"{tag} {ex['src']}\"  # Prefix source with language tag\n\n            model_inputs = self.tok(source, max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\", )\n\n            labels = self.tok(text_target=ex[\"tgt\"], max_length=max_len, truncation=True, padding=\"max_length\",\n                              return_tensors=\"pt\", )\n\n            model_inputs[\"labels\"] = labels[\"input_ids\"].squeeze(0)\n            model_inputs = {k: v.squeeze(0) for k, v in model_inputs.items()}\n            model_inputs[\"forced_bos_token_id\"] = forced_bos_token_id\n\n            yield {k: torch.tensor(v) if isinstance(v, list) else v for k, v in model_inputs.items() }\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--split\", type=str, default=train_data,  # required=True,\n                        help=\"TSV split file path\")\n    args = parser.parse_args()\n\n    print(f\"ðŸ“‚ Using split file: {args.split}\")\n\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name)\n\n    # # Save pretrained model\n    # tokenizer.save_pretrained(f'{base_path}nllb_200_distilled_600M_tokenizer/')\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name,\n                                              cache_dir=f'{base_path}nllb_200_distilled_600M_tokenizer/')\n\n    # # Load\n    # tokenizer = Tokenizer.from_file(f'{base_path}nllb_200_distilled_600M_tokenizer/')\n\n    stream = TSVStream(args.split, tokenizer)\n\n    print(\"ðŸ” Sanityâ€‘checking first few samples â€¦\")\n    for i, ex in enumerate(stream):\n        print(f\"\\nSample {i + 1}:\")\n        for k, v in ex.items():\n            if torch.is_tensor(v):\n                print(f\"  {k}: {tuple(v.shape)}\")\n            else:\n                print(f\"  {k}: {v}\")\n        if i >= 4:\n            break\n\n# if __name__ == \"__main__\":\n#     main()\n# â”€â”€â”€ Load tokenizer & model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained_tokenizer_name).to(device)\n# â”€â”€â”€ Load Datasets from TSVs using your streaming parser â”€â”€â”€â”€â”€â”€â”€â”€\ntrain_tsvstream = list(TSVStream(train_data, tokenizer))\nval_tsvstream = list(TSVStream(val_data, tokenizer))\ntrain_ds = Dataset.from_list(train_tsvstream)\neval_ds = Dataset.from_list(val_tsvstream)\nprint(\"âœ… Dataset sizes:\")\nprint(\"   âž¤ train:\", len(train_ds))\nprint(\"   âž¤ eval :\", len(eval_ds))\n# â”€â”€â”€ Data Collator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n\n# make sure output directory is created\nos.makedirs(output_dir, exist_ok=True)\n# â”€â”€â”€ Training arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntraining_args = Seq2SeqTrainingArguments(output_dir=output_dir, num_train_epochs=num_epochs,\n                                         per_device_train_batch_size=batch_size,\n                                         per_device_eval_batch_size=batch_size,\n                                         eval_strategy=\"epoch\", save_strategy=\"epoch\",\n                                         save_total_limit=3, predict_with_generate=True,\n                                         fp16=torch.cuda.is_available(),\n                                         logging_dir=\"./logs\", logging_steps=100,\n                                         report_to=\"none\",  # disables W&B\n                                         load_best_model_at_end=True,\n                                         metric_for_best_model=\"eval_loss\", greater_is_better=False, )\n\n# â”€â”€â”€ Trainer setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrainer = Seq2SeqTrainer(model=model, args=training_args, train_dataset=train_ds, eval_dataset=eval_ds,\n                         tokenizer=tokenizer, data_collator=data_collator, )\n# â”€â”€â”€ Train model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrainer.train()\n# â”€â”€â”€ Save final model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"\\nâœ… Fine-tuned model saved to: {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:14:11.025381Z","iopub.execute_input":"2025-09-09T15:14:11.025948Z","execution_failed":"2025-09-09T18:01:58.524Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033615eca63645b497146749f1a0b919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ebcda796fb94a209f9c64ff12f554d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edca7f2da92b403f95352d719491907f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05857ae0694a493ba817a456b5779c9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc093a4cd24243368c191d84643b1858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2dc831136b43a69ba0c4a0ba95b060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11290e1777646d8b8b4f99641d672ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0118697f27994abcb2fa8fe7b7ddb1b7"}},"metadata":{}},{"name":"stdout","text":"âœ… Dataset sizes:\n   âž¤ train: 122579\n   âž¤ eval : 11681\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2398038825.py:136: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(model=model, args=training_args, train_dataset=train_ds, eval_dataset=eval_ds,\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2968' max='766200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2968/766200 1:43:32 < 444:02:47, 0.48 it/s, Epoch 0.39/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"!pip install pandas torch logging datasets transdormers evaluate nltk \n# 1. Import Modules\nimport pandas as pd\nimport torch\nimport logging\nfrom datasets import Dataset\nfrom transformers import (\n    T5ForConditionalGeneration, T5Tokenizer,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n)\nimport evaluate\nimport nltk\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom google.colab import files\n\nprint(\"Upload CAC_DA_Amh_Eng_train.tsv file\")\n#uploaded = files.upload()\n#file_name = list(uploaded.keys())[0]\n\nnltk.download('punkt')\nlogging.basicConfig(level=logging.INFO)\n\n# 2. Load and Clean TSV Dataset\ndf = pd.read_csv(file_name, sep='\\t', header=0)\ndf = df.rename(columns={df.columns[0]: \">>amh<<\", df.columns[1]: \">>eng<<\"})\ndf_clean = df.dropna(subset=[\">>amh<<\", \">>eng<<\"])\ndf_clean = df_clean[df_clean[\">>amh<<\"].str.strip().astype(bool) & df_clean[\">>eng<<\"].str.strip().astype(bool)]\n\n# 3. Convert to Hugging Face Dataset and Split\ndataset = Dataset.from_pandas(df_clean.reset_index(drop=True))\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\n# 4. Tokenization Setup\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nprefix = \"translate Amharic to English: \"\n\ndef preprocess_function(example):\n    input_text = prefix + example[\">>amh<<\"]\n    target_text = example[\">>eng<<\"]\n    model_inputs = tokenizer(input_text, max_length=128, padding=\"max_length\", truncation=True)\n    label_ids = tokenizer(target_text, max_length=128, padding=\"max_length\", truncation=True)[\"input_ids\"]\n    model_inputs[\"labels\"] = [token_id if token_id != tokenizer.pad_token_id else -100 for token_id in label_ids]\n    return model_inputs\n\ntokenized_ds = dataset.map(preprocess_function, batched=False)\n\n# 5. Load Model & Setup Data Collator\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# 6. BLEU Metric Load\nbleu = evaluate.load(\"bleu\")\nsmoother = SmoothingFunction().method4\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # Replace potential out-of-range token IDs in predictions with pad_token_id\n    preds[preds == -100] = tokenizer.pad_token_id\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 back to pad_token_id for decoding\n    labels = [\n        [(token if token != -100 else tokenizer.pad_token_id) for token in example]\n        for example in labels\n    ]\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Ensure references are lists of strings for BLEU computation\n    decoded_labels = [[lab] for lab in decoded_labels]\n    return bleu.compute(predictions=decoded_preds, references=decoded_labels)\n\n# 7. Training Configuration\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5-amharic-eng-final\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=5,\n    predict_with_generate=True,\n\n)\n\n# 8. Trainer Setup\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# 9. Train the Model\ntrainer.train()\n\n# 10. Save Final Checkpoint\nmodel.save_pretrained(\"t5-amharic-eng-final\")\ntokenizer.save_pretrained(\"t5-amharic-eng-final\")\n\n# 11. Run Sample Inference\nsample_inputs = df_clean[\">>amh<<\"].sample(10, random_state=42).tolist()\ninputs = tokenizer([prefix + s for s in sample_inputs], return_tensors=\"pt\", padding=True).to(model.device)\noutputs = model.generate(**inputs)\ntranslations = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\npd.DataFrame({\"Amharic\": sample_inputs, \"Predicted English\": translations}).to_csv(\n    \"sample_predictions.csv\", index=False\n)\nprint(\"âœ… Sample predictions saved to sample_predictions.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T18:01:58.533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}